---
title: "microhaplot walkthrough"
author: "Thomas Ng and Eric C. Anderson"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{microhaplot walkthrough}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

`microhaplot` comes pre-loaded with two small, real, example data sets, 
`fish1.rds` and `fish2.rds`, to get you started learning about its 
functionality.
This vignette will walk you through the process of viewing these example data 
sets and will introduce you to many of the features of `microhaplot`.
Each data set consists of microhaplotypes derived from
amplicon sequencing of a marine species, and they represent subsets of
data that have been previously employed in full analyses.

The data that we will be exploring will be available
after installation of the R package and the Shiny app. Once the R package
`microhaplot` is installed, the Shiny app can be installed like this:
```r
# provide a directory path to host the microhaplot Shiny app
shiny_dir <- tempdir()
microhaplot::mvShinyHaplot(shiny_dir)
``` 
where `tempdir()` should be replaced by a location on your hard drive where 
you want the Shiny app installed. 

Once it is installed, the Shiny app is run like this:
```r
app.path <- file.path(shiny_dir, "microhaplot")
microhaplot::runShinyHaplot(app.path)
```

# About the example data
The data set in `fish1.rds` contains microhaplotypes from 22 amplicons in 436 
individuals of one population group. `fish2.rds` consists of data from 14
amplicons in 288 individuals from two populations. The actual sequencing runs 
that produced the data contained many more loci, but we have subsampled the
original data sets in order to make the data portable. The motivation behind
supplying two datasets is to show that, despite both being derived from 
amplicon-sequencing reads, there is great variation in read depth between the
two data sets, and this will affect the reliability of called genotypes.  

# Microhaplot layout

The `microhaplot` shiny viewer is split into three horizontal "panels". From top to bottom, they are:

## **Interaction tabset** (top)  
These are the three tabs that live at the top of the page with the light
blue background.  Any one of the three: "> field selection", "+ read criteria", and "+ locus annotation"
can be selected.  Whichever one is selected determines the content of the panel directly beneath---the
user input panel, the panel with the light gray background.  

  a. **Field selection tab:**<br>  
    <img src="field_selection.png" title="read criteria tab" width="80%" height="70" />
    <br><br> 
    If the "> field selection" is selected, then the user input panel will include
    dropdown boxes that let you choose a group, locus, or individual(s) that you wish
    to focus on (this choice affects what is shown in different plots and outputs,
    etc.)<br><br>
    The top choice in each dropdown box is "ALL" which indicates that results
    for all (groups, individuals, or loci) are to be portrayed at once. This tab also
    includes "backwards-and-forwards" buttons, "<" and ">", that sit
    immediately to right of the **Indiv** and the **Locus** dropdown boxes,
    and which allow you to step backward or forward through the content
    of the dropdown boxes. These "backwards-and-forwards" buttons are very
    useful when you want to work your way through each locus, one at a time,
    to choose those that do (and do not) appear to be reliable enough to
    include in a panel of amplicons. (*Note:* the location and relative position
    of each button might change pending on the width of the viewing window.)
  
  b. **Read criteria tab:** <br>  
      <img src="read_criteria_tab.png" title="read criteria tab" width="80%" height="70" />  
      <br>
    In the "+ read criteria" tab, you are given the choice to define the top $n$ alleles, minimum read depth and minimum allelic ratio.  These values specify a series of filters that microhaplotypes must satisfy to pass through the analysis. It is recommend that adjust these filters in a particular order, as follows:
    
      i. `Top n Alleles` defines the upper number of distinct microhaplotype sequences
        (in a single individual's locus) to be kept, starting from the highest-read-depth haplotype and proceeding to haplotypes found in lower read depths. Ideally, the top n alleles should match the ploidy
        of the species you study. So for a diploid organism, the *top n alleles* should be set to 2. For a
        tetraploid, the *top n alleles* should be set to 4.
    
      ii. `Min Total Read Depth` or _Minimum Total Read Depth_ defines the
        minimal total number of reads from the "Top n alleles" required for a locus to pass filter within
        an individual.  It can only take values greater than or equal to 0.   
        <br>For example, if `Top n Alleles` is set to 2, and `Min Total Read Depth` is set to 15, then if Iindividual A's two highest read depth haplotypes at Locus 1 are CCA with 10 reads and GGA with 7 reads, then the two haplotypes retained from Individual A at Locus 1 are CCA and GGA.  
        <br>By contrast, if Individual B's two highest-read-depth haplotypes at Locus 1 are GTC with 6 reads and GGA with 5 reads, then, because the total number of reads of those two haplotypes is 11, which is less then 15, none of Individual B's haplotypes at Locus 1 will pass this criteria, therefore no genotype will be reported for Individual at Locus 1.
        
      iii. `Min Allelic Ratio` or _Minimum Allelic Ratio_ defines the minimum allelic ratio that a haplotype needs to pass filter. It must be between 0 and 1. 
        For a particular locus, L, of individual I, the allelic ratio of haplotype X is defined as the ratio of the observed read depth of haplotype X at L in I to the read depth of the haplotype with highest read depth at L in I.  
        <br>The `Min Allelic Ratio` filter removes from consideration those reads of an allele/haplotype that occur in low depth relative to the highest-read-depth allele.
        It is important to understand that the _allelic ratio_ is a quantity that applies to each distinct haplotype (allele) at a locus within an individual, and it is relative to the haplotype/allele with the highest read depth.
        Accordingly, the values of allelic ratios you should expect to see in an individual that is truly homozygous or heterozygous are as follows:
        
        > In a **homozygous** individual, the highest read depth allele should be the one carried in n copies by the n-ploidy individual.  Therefore you expect that true allele/haplotype to have a large read depth and an allelic ratio of one while other alleles/haplotypes should have low allele-specific read depth and allelic ratio close to zero.  
        
        > In a truly **heterozygous** diploid, the two alleles/haplotypes carried by the individual will, ideally, each have a large and similar number of reads, which means that their allelic ratios should be at or near one. The _Minimal Allelic Ratio_ defines a filter that removes reads from haplotypes with allelic ratio less than the minimum.  

        Once you set up these three criteria, you  _apply_ them to see their effects on the
        currently displayed data visualization with the "apply" button. You can also _save_
        these criteria if you want the values to "stick", by clicking the "save" button. 
        
    iv. Additional options. When selecting "All loci", additional filtering options will be available at the right hand side of the criteria box. Details for each of the options are displayed if you hover your pointer over it:
        * `overrides only as min baseline`: when selected and _apply_, all of the criteria (top.n.2, min.tot.rd, min.ar) will replace each of the corresponding locus criterion if and only if the `all` loci's criterion is greater than the value of a single locus level. It is useful when you want to set a minimal baseline for every single locus. For example, if minimal total read depth for locus A, B, and `All` loci is set to 5, 11, 10, respectively, the minimal total read depth for locus A will be updated to 10 when this option is selected and _apply_.
        * `overrides all params`: This option, when selected and _apply_, will set every single locus criteria into the current `All` loci criterion setting. Be wary that there's no way to undo the overwritten process once this option is selected and applied. For example, if minimal total read depth for locus A, B, and `All` loci is set to 5, 11, 10, respectively, the minimal total read depth for locus A and B will both be updated to 10 when this option is selected and _apply_.
        * `relies on local locus`: Instead of filtering based on the current displayed filter criteria, the microhaplotypes are filtered based on the local parameters that are defined on a locus-by-locus level. For example, if minimal total read depth for locus A, B, and `All` loci is set to 5, 11, 10, respectively, all microhaplotype for locus A and B must pass minimal total read depth of 5 and 11, respectively.
      
  c. **Locus annotation:**  
    <img src="locus_annotation.png" title="read criteria tab" width="80%" height="70" />  
    This tab is for users to keep track of whether a certain locus should be retained based
    on its quality or other attributes. This is most useful when a particular locus has
    been chosen under the "field selection" panel. There is also a small textbox that
    lets you insert any comment regarding the locus group. You must press the **save button**
    to submit this information about each locus. All of this locus-annotation information
    can be retreived under the table selection for "locus annotation" under the `Output` tab (see below).
    

> **Ways to access saved filter features and annotation in flat format:** All of the saved parameters such as minimal total read and annotation are read and saved as an RDS file in the same directory path as the shiny app. The name of the annotated file carries with a suffix - "_annotate.rds". For example, for fish1.rds data, the annotate file name is fish1_annotate.rds. Another way to retrieve the info is by downloading the "locus annotation" table from `Table` tab at the bottom pane.

## **Main body panel** (center)
This panel occupies most of microhaplot's screen real estate.  It is where the actual
plots, tables, and other results are rendered.  What gets shown here depends on which main body panel
has been chosen - located right at the bottom.  That choice is made using the *main panel choice tabset* described below.

## **Panel choice tabset** (bottom)
The tabset is always down at the bottom of the screen, and the choices on this tabset allow you to choose what contents to be displayed in the main body panel:     
<img src="main_tab.png" title="main tab" width="60%" height="40" />
<br>
   
  a. *Data Set:*  provides a dropdown box in the main panel that you use to choose which data set you want to explore.
    The choices in the dropdown box are taken from the `.rds` files that are in the `app.path` 
    directory described above.
    When you start microhaplot after a clean install, the default dropdown choice is `fish1.rds`, and the
    other possible choice is `fish2.rds`.      
  b. *Summary Info: * displays condensed graphical summary of the characteristics of microhaplotype reads by groups, individuals, or loci.  These are particularly useful for overviews when the "ALL" choice is given for individuals and loci in the _field selection tab_.     
  c. *Criteria Cutoff: * contains:   
      i. `Global Scope`: This section displays the summary and distribution regarding the read depths and allelic balance ratio to assist the user in setting up an estimate value of each filter cutoff.
      <br>We present histograms of read depths and allelic balance ratios across one (or ALL) loci. If only a single locus is selected, the histogram of read depths and allelic balance ratios are shown for each allele, as well.    
      <br>At the bottom, we also provide summary of the number of alleles and the number of individuals with callable genotypes (given the filters) available at different cutoffs for the minimum total read depth and the minimum allelic ratio.
      ii. `Quality Profiling`: This segment displays individuals or loci that may carry more than expected number of large read depth microhaplotypes, possibly under the event of sample cross-contamination or targeting paralogous or repetitive loci. To highlight those problematic loci or individuals, the minimum read depth filter (per haplotye) is used instead of minimum total read depth.   
      <br>The upper plot shows the spread of individuals or loci that might bring potential problems in getting exact genotype calls. The details of those items can further examined in the bottom table.
  d. *Genotype Call: * offers a more intimate look and analysis on how genotype is being called at a single locus level.    
    i. `AR refinement` displays the read depth relationship between the most common microhaplotype and other non-primary microhaplotype in the form of a biplot. This space also provides you the tools (found on the left) to fine-tune allelic ratio parameters (such as defining the maximum allelic ratio for homozygous individuals and minimum allelic ratio for heterozygous individual) so that individuals with questionable allelic balance ratio will be opt out for genotype call. At the bottom, you will also find the distribution of read depths and the allelic depth ratios of different haplotypes in relation to whether such haplotype is being included in genotype call and final genotype class - homozygous or heterzygous - regarding the individual.

    ii. `Summaries` shows haplotype frequencies within each group, and a visual interactive summary of conformance to Hardy-Weinberg proportions of individual loci, and also portrays the position of these variants within the sequence.

  e. *Table: * provides a place to get a number of tabular views of the data that are sortable and searchable.  This can be very useful when you want to dig into the nitty-gritty details of the short read data.  It also allows you to download filtered and called data and genotypes for later analysis.

  f. *About (microhaplot): * provides a little background about microhaplot and how to cite it and whom to contact about it.

With that background we are ready to start exploring the example data.

# Mission one: An overview of loci

The first thing we are going to do is get an overview of the data with no filtering, focusing on
different loci.  To do this, we choose *Summary Info --> By Locus*.  We also want to
make sure that we are starting from the defaults in ">field selection" (ALL group, ALL loci, ALL individuals),
and under "+read criteria" you want to ensure that "Minimal Read Depth" and "Minimal Allelic Ratio" are
both set to 0.  (If you had changed these before, then, once you set them back to 0, you should hit the "apply"
button to apply the changes.)

Choosing *Summary Info --> By Locus* will give you four plots.  By default, only the first 15 loci are
represented (the reason for this is that if you have many loci, you don't want to end up waiting for an
eternity to have the final plots rendered.)  You can change the number of loci shown using the
"Display" dropdown menu that lives just beneath the user input panel.  You can also choose to go through
the loci, 15 at a time by choosing which page to go to using the "Page" selector to the right of the
"Display" selector.  

The four figures shown in this By Locus summary are, from left to right:

1. The distribution of the number of haplotypes observed per individual.  Recall that these are subject to the
current filters, of which there are none, so each haplotype, even if it is only observed at a read depth of
1 read in an individual is counted as observed.  This plot is a bubble plot that essentially is giving
histogram-like information.  The y-axis has different locus on each line.  The x-axis is the number of unique haplotypes
and the size and shading of the bubbles indicate how many individuals in the data set had the given
number of unique haplotypes.    It is particularly revealing to focus on locus `tag_id_1166` (5th locus in main body panel).  Most individuals
have a large number of different haplotypes, likely occurring at very low read depths.  At any rate, this
plot makes it clear that there are a lot of low-read-depth sequences that are superfluous and incorrect.

1. Moving to the right is another figure that has loci arranged on the y-axis and number of haplotypes on the
x-axis.  However, in this case, the number is counting the total number of unique haplotypes observed at the
locus when only the two highest-read-depth haplotypes are recorded per individual.

1. The third plot is a plot with light blue points indicating the fraction of individuals that have
sufficient read depth to allow a genotypes to be called given the current settings of the read depth and allelic ratio filters.  
Obviously since there is filtering, every locus is going to have a genotype called at every individual.

1. The final plot (the furthest right) just shows violin plots expressing the distribution of read depths at
each locus across the individuals.  Notice that the x-axis is on a log scale.  The plus denotes the median read
depth.  

Now that we know what all of these plots mean, we can interpret what we see.  

We note that most loci have a
read depth greater than 20 at all the individuals.  Also there is a huge amount of
variation across loci in the number of recorded haplotypes per individual.  Since any
diploid individual will have at most two distinct haplotypes at a locus, most of these haplotypes
are spurious, and we can hope that most of them are at very low read depths and will be filtered out.
We will investigate that now.

## Let's apply some filters

Here is the moment we have all been waiting for.  We have been looking at our data in a totally unfiltered state---effectively regarding
a haplotype with 1 read to be on an equal footing with one having, say, 1000 reads in an individual.  There are two, simple, main
filtering methods in microhaplot.  The first is to discard haplotypes from individuals if they occur in fewer than a certain number
of reads (The "Minimal Read Depth" criterion).  The second is to discard haplotypes from individuals if their read depth is less than a given fraction of the haplotype
with the highest read depth at the locus in the individual (The "Minimal Allelic Ratio" criterion).  Choose 25 for the Minimal Read Depth and
slide the bar to 0.2 for the Minimal Allele Ratio, and then hit "Apply".  

Once those filters take effect we find that most individuals are observed to carry one or two haplotypes at each locus.  Some loci
have more haplotypes that that observed in some individuals.  We will want to check up on those later.  Some of the loci clearly did
not sequence well.  For example `tag_id_1511`.  (hint: look for violin plot w/ skewed far left distribution) To see an expanded view of a few loci, you can highlight (with a click drag) their
rows in the left hand figure and then double click those to get a zoomed in view.  To zoom back out, to be able to see all
the loci, just double click the body of the left figure.  

# Mission Two: An overview of individauls

We can take those same filters we just applied and get a good overview of our sequencing data on an
individual basis.  Go ahead of choose *Summary Info-->By Individual*.  from the main panel choice
tabset on the bottom of the screen.  The initial screen that comes up includes only 15 of the individuals.  We can change that
by choosing "ALL" in the "Display (indiv/pg) " dropdown.

This "By Individuals" summary view of the data provides four different plots.  From left to right they
are:

1.  The ratio of the haplotype with 2nd highest read depth to the haplotype with highest read depth.  
In this plot, the y-axis denotes individuals, the x-axis represents the ratio (it goes from 0 to 1)
and each locus is represented as a bubble.  The size of the bubble is scaled to some function of
read depth, but it is hard to place that on an absolute scale.  The main take home message
from this plot is that for most individuals at most loci the two highest-read depth haplotypes have
fairly similar read depths.  For example a ratio of 0.75 indicates that if the haplotype with highest
read depth at a locus in an individual was 100 reads, then, the haplotype with second highest read
depth was 75 reads.  

1. The number of haplotypes observed in each individual.  This is a bubble plot that expresses the relative
number of loci at which 1, 2, 3, etc haplotypes were observed after filtering.  

1. The number of loci within the individual with sufficient read depth to call a genotype (given the current read depth and allelic ratio filters).

1. The distribution of read depths at all the haplotypes that were not filtered out at all the loci in the individual.
These are portrayed as violin plots with individual
values as dots within them.


It should be noted that the colors of the bubbles on the left-most graph indicate the different
groups that the individuals fall into.  This is particularly useful when individuals from different
populations or species are in your dataset.  In the case of our example data, we have merely
assigned half of the individuals to be from "pop1" and the other half from "pop2".

## Spotting some dodgy individuals

This overview of all the individuals gives us a good chance to spot a few problem fish
right off the bat.  Just scanning down the fraction of loci that the individuals might be
called at, it is clear that s45 and s277 had limited sequencing success.  That is clear by
looking at the distribution of read depths across loci.  Perhaps even more intriguing is
individual s288 that seems to have pretty reasonable read depths, but very poor allelic
balance.  

## Looking at the raw counts for s288

Another way to look at the data is through the tabular format.  If we want to see tabular data for individual s288, choose "Output" from the main panel choice tabset.   By
default this will show a few lines of the "Observed variants (unfiltered)" table.
You can change which table is viewed using the "Select Table" dropdown, but unfiltered
observed variants is great for now.  

In the search box on the right, enter `s288` and hit return. Note: search box is not case sensitive.  This will filter the table to only
those rows that include s288.  Once that is done, click on the "locus" column heading to
sort entries by locus.  Voila! Now you can peruse all the observed variants and their read
depths at this individual s288.  Where it says "Show 15 entries" on the left hand side just above the table you should change the "15"
to "All".  This will give you a single scrollable window of all the entries for s288.  Note that
the column "Haplo" gives the sequence of the reads at the variable positions.  These are
essentially the haplotypic states of the reads.

Going through the data for 288, it is clear that there will be some genotypes that are not
straightforward to call.  For example, at locus `tag_id_879` there are three different
haplotypes with read depths over 50:  128 reads of CCCTCTG, 100 reads of CCCTCAG, and
57 of GCCCCTG.  That is clearly going to be difficult to genotype.  Perhaps this
individual has contaminated DNA or represents two individuals that were barcoded equivalently.
It probably should be tossed out of the final data set.

## Raw count data for a less dodgy individual?

So, the data for s288 was a little disquieting, but recall that was an individual that
looked pathological from the very beginning.  So, what if we look at the raw count
data for an individual that doesn't look so dodgy?  This will be a good exercise to
build up some intuition about how genotyping might be done with microhaplotype read
data.

Let's have a closer look at individual s95.  From the overview, this is an individual with
reasonably clean, abundant reads.  We can filter on this individual merely by replacing
"s288" in the search box with "s95".  A cursory glance at the results for this individual
show us what appears to be an individual that will be very easy to genotype:  there are never
more than two haplotypes at high read depth but there is always at least one haplotype
at high read depth.  When two haplotypes are at high read depth, their read depths
are largely quite similar.  

# Looking over different loci

When we are interested in assessing how well each locus behaves in reporting microhaplotypes, it is time to turn to the "Filter Analysis" panel choice.
First, go ahead and choose *Filter Analysis --> Broad Summary*.  This view gives you a tabular
perspective on the outcome of different filtering choices on the total number of haplotypes
recorded in the data set.  The field selection choices that are present are applied here. So,
if ALL loci are selected, then it counts up the total number of haplotypes typed and the total
number of individuals typed at all loci.  If you select just one locus, the results reflect that
one locus.

With that in mind, choose the first locus in the dropdown menu: `Plate_1_A01_Sat_GW603857_consensus` under "> field selection"
and see how that changes the broad summary.  Note that across a broad range of the two filtering
options (minimum read depth and minimal allelic balance) there are four haplotyes in total discovered.

For a more detailed view of the information associated with this locus (especially relative to the
filtering choices) you can select *Filter Analysis --> Criteria Breakdown*.  This view shows
a histogram of read depths and allele balance ratios for reads assigned to each indiduals, and it shows
these when broken down by haplotype.  The filtering choices in effect appear as dashed red lines.

To see how the inferred haplotypes look in terms of haplotype frequencies, and also how the genotypes
look in terms of Hardy-Weinberg equilibrium, you can choose *Filter Analysis --> Microhap Summaries*.
This view consists of four figures.  The first, in the upper left simply shows the frequencies (and the total read depth) of
different haplotypes.  The plot in the upper right shows the relationship between the
observed frequency of different genotypes and the expected frequencies under Hardy-Weinberg equilibrium.
The individuals used in creating these summaries depends on which "Group" is chosen.  In this case we have
"ALL" chosen, and that is fine because the two groups are essentially identical, genetically.  However,
if we were dealing with groups that were genetically differentiated, we would not want to assess
conformance to Hardy-Weinberg proportions of a mixture of those different groups!  In such a case
it is worthwhile to look at one group at a time.

The expected number of different genotypes is shown by the oulines of circles and the observed number by the
filled, colored circles.  Green are homozygotes, orange are heterozygotes, and it should be relatively
self-explanatory.  There is not a scale, but if you click in the center of any of the genotypes with
observed (non-zero) counts, you will be told (in the upper left of the panel) what the expected and
observed numbers were for that genotype.  These plots are not meant to provide a defensible test
of departures from Hardy-Weinberg equilibrium, but do allow the user to diagnose loci that are
grotesquely far out of Hardy-Weinberg equilibrium.

Below the haplotype frequencies and HW conformance plots you will find a simple bubble plot
expressing haplotype frequencies in the different groups.  In the case of the example data there
are only two different groups and they have very similar allele frequencies.  This plot becomes more
useful when one is comparing allele frequencies across many different groups.

Finally, you may need to scroll down to see the final figure in this display.  It is a representation of the
haplotype sequences, their frequencies, and the positions of the variants within them along each amplicon.

# Output of data

We have already witnessed how to use the *Output* panel to look at the raw data.  It is also possible to
download filtered, unfiltered, and processed data from this view using the "Download Data" button. These
saves output in CSV files.

# Locus annotation

The program can be used to annotate different loci (and assign filtering criteria to them). This
annotation can be done using the "+locus annotation" interaction tabset, and the results saved
by downloading the locus annotation data from the *Outputs* panel.

### importing external input data
